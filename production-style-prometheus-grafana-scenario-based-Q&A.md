# Real-World Production Scenario-Based Interview Questions

## Prometheus & Grafana ‚Äì STAR Format Answers

---

## ‚≠ê Scenario 1: Sudden Spike in CPU Usage Not Visible in Dashboard

‚ùì Interview Question:
"Tell me about a time when production CPU usage spiked but your dashboard did not reflect the issue immediately."

### ‚úÖ STAR Answer

**S ‚Äì Situation:**
In our Kubernetes production cluster, users reported application slowness. However, our Grafana dashboard was showing normal CPU metrics.

**T ‚Äì Task:**
I had to identify why the monitoring system was not reflecting real-time CPU spikes and ensure accurate visibility.

**A ‚Äì Action:**

* Checked Prometheus scrape interval (was set to 60s).
* Verified node-exporter metrics collection.
* Identified that short CPU bursts were being averaged out.
* Reduced scrape interval to 15s.
* Modified PromQL query from `avg(rate(container_cpu_usage_seconds_total[5m]))` to `rate(container_cpu_usage_seconds_total[1m])`.
* Updated Grafana panel refresh interval.

**R ‚Äì Result:**
Dashboard started reflecting CPU spikes accurately. We detected a memory leak in one microservice and resolved it before SLA breach.

---

## ‚≠ê Scenario 2: Alerts Not Triggering During Production Outage

‚ùì Interview Question:
"Explain a time when Prometheus alerts failed during a production outage."

### ‚úÖ STAR Answer

**S ‚Äì Situation:**
During a critical API outage, no alerts were triggered in Slack.

**T ‚Äì Task:**
Investigate why Alertmanager did not notify the team.

**A ‚Äì Action:**

* Checked Prometheus alert rules using `/alerts` endpoint.
* Found alert firing state was active.
* Verified Alertmanager config.
* Discovered misconfigured Slack webhook URL.
* Implemented alert routing and fallback email notification.
* Added synthetic alert testing in CI pipeline.

**R ‚Äì Result:**
Alerts were restored within 30 minutes. Introduced redundancy in alert channels.

---

## ‚≠ê Scenario 3: High Cardinality Causing Prometheus Crash

‚ùì Interview Question:
"Have you faced Prometheus performance issues due to high cardinality?"

### ‚úÖ STAR Answer

**S ‚Äì Situation:**
Prometheus server memory usage kept increasing and eventually crashed.

**T ‚Äì Task:**
Identify root cause and stabilize monitoring system.

**A ‚Äì Action:**

* Checked `tsdb status`.
* Identified high cardinality labels (`user_id`, `session_id`).
* Removed dynamic labels from instrumentation.
* Applied relabel_configs.
* Implemented recording rules for aggregation.
* Enabled retention policy tuning.

**R ‚Äì Result:**
Reduced time series count by 70%. Memory stabilized and no further crashes.

---

## ‚≠ê Scenario 4: Grafana Dashboard Showing No Data After Deployment

‚ùì Interview Question:
"What would you do if Grafana shows no data after a new deployment?"

### ‚úÖ STAR Answer

**S ‚Äì Situation:**
After deploying a new version of an application, Grafana panels displayed "No Data".

**T ‚Äì Task:**
Restore observability quickly.

**A ‚Äì Action:**

* Checked Prometheus target status (`/targets`).
* Found new pod labels changed (`app=v2`).
* Existing PromQL query was filtering `app=v1`.
* Updated dashboard queries.
* Implemented label consistency standards in CI.

**R ‚Äì Result:**
Metrics visibility restored in 10 minutes. Prevented future dashboard breakage.

---

## ‚≠ê Scenario 5: Disk Space Exhaustion Due to Prometheus Retention

‚ùì Interview Question:
"Describe a production incident where Prometheus storage caused issues."

### ‚úÖ STAR Answer

**S ‚Äì Situation:**
Production monitoring node ran out of disk space, affecting metric ingestion.

**T ‚Äì Task:**
Restore system and prevent recurrence.

**A ‚Äì Action:**

* Checked `/var/lib/prometheus` usage.
* Reduced retention from 30d to 15d.
* Enabled remote_write to long-term storage (S3 via Thanos).
* Implemented storage monitoring alert.

**R ‚Äì Result:**
Freed 60% disk space. Long-term metrics preserved externally.

---

# üî• Advanced Enterprise-Level Scenarios

1. Multi-cluster Prometheus federation design
2. Thanos/Cortex for HA and long-term storage
3. Prometheus HA pair setup with external labels
4. Alert fatigue reduction strategy
5. SLO/SLI-based alerting implementation
6. Blackbox exporter for synthetic monitoring
7. Grafana RBAC & multi-tenant dashboard isolation
8. Incident postmortem with monitoring gap analysis

---
